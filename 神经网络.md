#### 图神经网络基础知识

稀疏矩阵的存储方式

0 1 1 0 0
1 0 1 1 0
1 1 0 1 0
0 1 1 0 1
0 0 0 1 0

coc:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
rows=\[0 0 1 1 1 2 2 2 3 3 3 4\]
cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]

csr:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
indices=cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]
ind_ptr=\[0 2 5 8 11 12\]
ind_ptr\[i+1\] - ind_ptr\[i\]表示第i行中的数据个数，并且在indices中可读出每个数据的所在列


crc:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
indices=cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]
ind_ptr=\[0 2 5 8 11 12\]
ind_ptr\[i+1\] - ind_ptr\[i\]表示第i列中的数据个数，并且在indices中可读出每个数据的所在行

将图应用于机器学习的挑战
1.图缺乏一致的结构
2.每个节点并不是相互独立的，存在连接性
3.图的存储问题，连接性是用邻接矩阵来存储，如果图非常大存储不下来。由于邻接矩阵很稀疏，所以用稀疏矩阵来存储会更好，而稀疏矩阵在GPU上训练一直是个技术难题。
4.置换不变性，邻接矩阵任意交换行列，会导致邻接矩阵改变，他们其实节点关系不变。
节点顺序等变性

图神经网络与图核方法：？


![图神经网络分类](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665814700600.png)
循环图神经网络（RGNN）：假设图中的一个节点不断与其邻居交换信息/消息，直到达到稳定的平衡

卷积图神经网络（CGNN）：是通过汇总节点自身的特征Xv和相邻节点的特征Xu来生成节点v的表征形式
（1）基于谱方法
（2）基于空间方法

（1）节点分类：信息传递，最后的节点能够学习到离它很远邻居节点的特征
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818817431.png)
（2）图分类：通过池化层将图变成子图，从而学习到图高级特征，通过读出层综合这些特征
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818828585.png)

图自动编码器（GAE）：用与获取网络嵌入embedding向量,基于自编码器
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665817914133.png)
图自动编码器（VGAE）：图生成：基于变分自编码器（VGAE）

自编码器（AE）：自编码器属于嵌入与表征学习的一种，主要用于数据降维、压缩以及获取低维度表征等。
自编码器可以可以完美的恢复出输入，但是由于损失函数是直接度量重建样本与真实样本的底层特征之间的距离，而不是评价重建样本的逼真度和多样性等抽象指标，在数据生成任务上表现一般。
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818615987.png)
变分自编码器（VAE）：变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行**数据生成过程**的良好属性

时空图神经网络（STGNN）：同时考虑空间依赖性和时间依赖性。许多当前的方法将图卷积与RNN或CNN集成在一起以捕获空间依赖性，从而对时间依赖性进行建模。
用于：交通速度预测，驾驶员操作预期和人类行为识别等
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665819600788.png)

节点级任务：节点回归，节点分类  通过信息传播/图卷积来提取节点高级特征，使用多层感知器或softmax层作为输出层，GNN能够以端到端的方式执行节点级任务。
边级任务：边分类，连接预测  将来自GNN的两个节点的隐藏表征作为输入，可以利用相似度函数或神经网络来预测边的标签/连接强度。
图级任务：图分类(监督学习) 通过池化层将图变成子图，从而学习到图高级特征，通过读出层综合这些特征;图嵌入(无监督学习) 通过图自动编码器或变分图自动编码器

https://zhuanlan.zhihu.com/p/200888266
https://arxiv.org/pdf/1901.00596.pdf#page=18&zoom=100,65,949


#### 机器学习基础知识


![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666228669069.png)



DDP训练： https://zhuanlan.zhihu.com/p/178402798

超参优化：https://zhuanlan.zhihu.com/p/430580437


梯度下降算法：使用所有样本更新权重
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852140139.png)
小批量梯度下降算法：使用随机部分样本更新权重
随机梯度下降算法：使用随机一个样本更新权重

冲量梯度下降 vs. Nesterov算法
冲量(momentum)：对每一个权重进行冲量优化，该值越大，之前梯度方向对现在方向影响越大
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666855101383.png)

学习率衰减α 权重衰减λ
L2正则化的线性回归代价函数
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852466505.png)
L2正则化的线性回归梯度下降
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852497052.png)



学习率 
warm-up：在梯度下降方法中，如果采用样本数比较大的话，通常需要比较大的学习率，但在一开始的训练中，由于参数是随机初始化的，所以此时的梯度往往也很大，如果此时学习率也很大的话，训练将变得很不稳定。**为了提高训练的稳定性，我们在最初几轮迭代时，采用较小的学习率，等梯度下降到一定程度之后，再恢复到初始的学习率**。当预热过程结束的时候，再选择一种学习率衰减的方式来降低学习率。

学习率衰减的方式：[学习率调整策略](https://blog.csdn.net/weiman1/article/details/125647517?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166687203016800180688863%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166687203016800180688863&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125647517-null-null.142^v62^js_top,201^v3^control,213^v1^t3_control1&utm_term=lr_scheduler&spm=1018.2226.3001.4187)

模型的保存：
```python
#保存整个模型：
torch.save(model,'save.pt')
#只保存训练好的权重：
torch.save(model.state_dict(), 'save.pt')
#state_dict是一个字典，将每层与层参数的张量之间一一映射
```

模型的读取：
 ```python
torch.load('tensors.pt')

#Load all tensors onto the CPU
torch.load('tensors.pt', map_location=torch.device('cpu'))
 
#Load all tensors onto the CPU, using a function
torch.load('tensors.pt', map_location=lambda storage, loc: storage)
 
#Load all tensors onto GPU 1
torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
 
#Map tensors from GPU 1 to GPU 0
torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})

#model.load_state_dict()函数把加载的权重复制到当前模型的权重中去
model.load_state_dict(torch.load("save.pt"))  
```

#### transfomer与RCNN基础知识
swin transformer环境搭建过程中遇到了cuda/pytorch版本兼容问题，mmcv-full/mmdet版本兼容问题，环境搭建过程记录如下：
（1）pytorch安装
```shell
conda create --name openmmlab python=3.8 -y
conda activate openmmlab
conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.6 -c pytorch -c conda-forge
# 参照pytorch官网链接 https://pytorch.org/get-started/previous-versions/
```
（2）mmcv安装
```shell
pip install -U openmim
min install mmcv-full==1.6.0
# 参照 https://mmcv.readthedocs.io/en/latest/get_started/installation.html#
```
(3)mmdet安装
```shell
# 源码编译
git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
pip install -r requirements/build.txt
pip install -V -e . # or python setup.py develop

#pip安装
pip install mmdet -i https://pypi.tuna.tsinghua.edu.cn/simple
```
(4)apex安装--可选
```shell
# 源码编译
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir ./
```
(5)直接下载swin transformer官方文件夹比较方便得到configs文件
```shell
git clone https://github.com/SwinTransformer/Swin-Transformer-Object-Detection
```
(6)需要修改mmdet文件夹中的_init_.py文件 mmcv_maximum_version = 当前mmcv最高版本号
(7)新建checkpoints文件夹，下载对应config的pth文件

注意：**在demo文件夹中inference**会报错：
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1667301048750.png)
在主目录下则不会，原因未知

coco数据集标注详解：https://zhuanlan.zhihu.com/p/309549190
mmdetection框架详解：https://blog.csdn.net/qq_16137569/article/details/120929852

swin-transformer && Mask-RCNN
![](./attachments/11.10.pptx)

#### 强化学习基础知识
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/IMG_20221118_203728.jpg)
![IMG_20221118_203739](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/IMG_20221118_203739.jpg)

DQN中经验被大量浪费，采用经验回放方法
PG中每一次的经验使用并更新网络参数后，策略已经发生变化，以前的经验不能再被使用，因此采取PPO方法


![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1679386231056.png)

![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1679386147794.png)