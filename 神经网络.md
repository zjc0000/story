稀疏矩阵的存储方式

0 1 1 0 0
1 0 1 1 0
1 1 0 1 0
0 1 1 0 1
0 0 0 1 0

coc:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
rows=\[0 0 1 1 1 2 2 2 3 3 3 4\]
cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]

csr:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
indices=cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]
ind_ptr=\[0 2 5 8 11 12\]
ind_ptr\[i+1\] - ind_ptr\[i\]表示第i行中的数据个数，并且在indices中可读出每个数据的所在列


crc:
data=\[1 1 1 1 1 1 1 1 1 1 1 1\]
indices=cols=\[1 2 0 2 3 0 1 3 1 2 4 3\]
ind_ptr=\[0 2 5 8 11 12\]
ind_ptr\[i+1\] - ind_ptr\[i\]表示第i列中的数据个数，并且在indices中可读出每个数据的所在行

将图应用于机器学习的挑战
1.图缺乏一致的结构
2.每个节点并不是相互独立的，存在连接性
3.图的存储问题，连接性是用邻接矩阵来存储，如果图非常大存储不下来。由于邻接矩阵很稀疏，所以用稀疏矩阵来存储会更好，而稀疏矩阵在GPU上训练一直是个技术难题。
4.置换不变性，邻接矩阵任意交换行列，会导致邻接矩阵改变，他们其实节点关系不变。
节点顺序等变性

图神经网络与图核方法：？


![图神经网络分类](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665814700600.png)
循环图神经网络（RGNN）：假设图中的一个节点不断与其邻居交换信息/消息，直到达到稳定的平衡

卷积图神经网络（CGNN）：是通过汇总节点自身的特征Xv和相邻节点的特征Xu来生成节点v的表征形式
（1）基于谱方法
（2）基于空间方法

（1）节点分类：信息传递，最后的节点能够学习到离它很远邻居节点的特征
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818817431.png)
（2）图分类：通过池化层将图变成子图，从而学习到图高级特征，通过读出层综合这些特征
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818828585.png)

图自动编码器（GAE）：用与获取网络嵌入embedding向量,基于自编码器
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665817914133.png)
图自动编码器（VGAE）：图生成：基于变分自编码器（VGAE）

自编码器（AE）：自编码器属于嵌入与表征学习的一种，主要用于数据降维、压缩以及获取低维度表征等。
自编码器可以可以完美的恢复出输入，但是由于损失函数是直接度量重建样本与真实样本的底层特征之间的距离，而不是评价重建样本的逼真度和多样性等抽象指标，在数据生成任务上表现一般。
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665818615987.png)
变分自编码器（VAE）：变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行**数据生成过程**的良好属性

时空图神经网络（STGNN）：同时考虑空间依赖性和时间依赖性。许多当前的方法将图卷积与RNN或CNN集成在一起以捕获空间依赖性，从而对时间依赖性进行建模。
用于：交通速度预测，驾驶员操作预期和人类行为识别等
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1665819600788.png)

节点级任务：节点回归，节点分类  通过信息传播/图卷积来提取节点高级特征，使用多层感知器或softmax层作为输出层，GNN能够以端到端的方式执行节点级任务。
边级任务：边分类，连接预测  将来自GNN的两个节点的隐藏表征作为输入，可以利用相似度函数或神经网络来预测边的标签/连接强度。
图级任务：图分类(监督学习) 通过池化层将图变成子图，从而学习到图高级特征，通过读出层综合这些特征;图嵌入(无监督学习) 通过图自动编码器或变分图自动编码器

https://zhuanlan.zhihu.com/p/200888266
https://arxiv.org/pdf/1901.00596.pdf#page=18&zoom=100,65,949




![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666228669069.png)



基于云边协同的ROI视频编码系统

![基于云边协同的ROI视频编码系统通用架构](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666230684723.png)
流程：云服务器找到ROI区域反馈给边缘，边缘仅对每帧的ROI区域进行编码上传给云端分析。存在如下问题：
（1）edge-cloud 的反馈延迟非常大。

![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666232841254.png)
上图为edge-cloud 的反馈延迟非常大导致边缘设备存在一定的空闲时间。通常解决办法是在云端使用目标移动预测算法补偿反馈延迟；下图为云端预测错误，将错误的ROI区域传递给边缘。

（2）上传的帧是基于之前发现的ROI进行编码的，其中只包含旧目标，使得云端难以获得新目标。
改进1：上传低质量的非ROI区域给云端以获得新目标 vs.仍需要额外的带宽
B. A. Mudassar, J. H. Ko, and S. Mukhopadhyay, “Edge-cloud collaborative processing for intelligent internet of things: A case study onsmart surveillance,” in 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC). IEEE, 2018, pp. 1–6.
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666261047414.png)
Object Detection处理传入的帧并生成目标检测框，ROI prediction预测ROI区域以补偿反馈延迟。边缘Multi-QF MJPEG Compression根据云端的反馈对ROI和非ROI区域进行不同QF权值的编码。


改进2：将视频分为关键帧和非关键帧，关键帧（包含非ROI区域）全部上传给云端，非关键帧在边缘用于目标跟踪
假设出现了新目标，云端在关键帧上传并接收到反馈前，不能在非关键帧进行新目标的跟踪。
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666233536553.png)
H. Gu, Z. Ge, E. Cao, M. Chen, T. Wei, X. Fu et al., “A collaborative and sustainable edge-cloud architecture for object tracking with convolutional siamese networks,” IEEE Transactions on Sustainable Computing,2019.
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666250752135.png)
（1）初始化边缘Tracking Module和云端Rectification Module
（2）Tracking Module使用轻量级的KCF算法获取帧中ROI的区域，发送给边缘Evaluation Module（非关键帧）。周期性的发送给云端Evaluation Module（关键帧）。
（3）边缘Evaluation Module和云端Evaluation Module使用PCC方法评估ROI区域的准确性，若准确性小于设定的阈值，则向云端Rectification Module发送请求进行ROI区域矫正。
（4）云端Rectification Module通过卷积 Siamese 网络得到准确的ROI区域位置并将其发送给边缘，边缘Evaluation Module和云端Evaluation Module之后将此ROI区域作为基准用于评估准确性。


解决思路：（1）根据新目标出现的频率，动态调整发送关键帧间隔，以平衡带宽与准确率。将每个帧都定义为关键帧上传云端分析可获得最高准确率，但需要最高带宽资源。（2）需要新算法找到ROI区域的丢失并重新找到非关键帧里新目标的位置。

改进3：PETRI目标检测系统整体架构
PETRI: Reducing Bandwidth Requirement in Smart Surveillance by Edge-Cloud Collaborative Adaptive Frame Clustering and Pipelined Bidirectional Tracking

![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666235794503.png)

Adaptive Frame Clustering & Task Pipelining:为避免边缘长时间的空闲，通过多任务并行处理帧的上传。每一个任务包括一个关键帧和多个非关键帧。通过接收云端未被跟踪到的目标的比例来动态调整非关键帧的个数。该值越高关键帧的比例越高以提高准确率。

![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666236381272.png)
Bidirectional Edge Tracker:关键帧不经过它处理。非关键帧通过部署在该模块上的目标跟踪算法来跟踪目标的位置，该模块同时接收云端检测到的未被跟踪的新目标反馈信息（该信息是通过关键帧得到的）来重新跟踪到所有的目标轨迹。（得到非关键帧所有的ROI区域）

ROI-based Encoding:（1）直接上传所有的关键帧到云端分析；（2）将非关键帧的非ROI区域舍弃掉再上传给云端分析。




DDP训练： https://zhuanlan.zhihu.com/p/178402798

超参优化：https://zhuanlan.zhihu.com/p/430580437


梯度下降算法：使用所有样本更新权重
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852140139.png)
小批量梯度下降算法：使用随机部分样本更新权重
随机梯度下降算法：使用随机一个样本更新权重

冲量梯度下降 vs. Nesterov算法
冲量(momentum)：对每一个权重进行冲量优化，该值越大，之前梯度方向对现在方向影响越大
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666855101383.png)

学习率衰减α 权重衰减λ
L2正则化的线性回归代价函数
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852466505.png)
L2正则化的线性回归梯度下降
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1666852497052.png)



学习率 
warm-up：在梯度下降方法中，如果采用样本数比较大的话，通常需要比较大的学习率，但在一开始的训练中，由于参数是随机初始化的，所以此时的梯度往往也很大，如果此时学习率也很大的话，训练将变得很不稳定。**为了提高训练的稳定性，我们在最初几轮迭代时，采用较小的学习率，等梯度下降到一定程度之后，再恢复到初始的学习率**。当预热过程结束的时候，再选择一种学习率衰减的方式来降低学习率。

学习率衰减的方式：[学习率调整策略](https://blog.csdn.net/weiman1/article/details/125647517?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166687203016800180688863%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166687203016800180688863&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125647517-null-null.142^v62^js_top,201^v3^control,213^v1^t3_control1&utm_term=lr_scheduler&spm=1018.2226.3001.4187)



模型的保存：
```python
#保存整个模型：
torch.save(model,'save.pt')
#只保存训练好的权重：
torch.save(model.state_dict(), 'save.pt')
#state_dict是一个字典，将每层与层参数的张量之间一一映射
```

模型的读取：
 ```python
torch.load('tensors.pt')

#Load all tensors onto the CPU
torch.load('tensors.pt', map_location=torch.device('cpu'))
 
#Load all tensors onto the CPU, using a function
torch.load('tensors.pt', map_location=lambda storage, loc: storage)
 
#Load all tensors onto GPU 1
torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
 
#Map tensors from GPU 1 to GPU 0
torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})

#model.load_state_dict()函数把加载的权重复制到当前模型的权重中去
model.load_state_dict(torch.load("save.pt"))  
```


swin transformer环境搭建过程中遇到了cuda/pytorch版本兼容问题，mmcv-full/mmdet版本兼容问题，环境搭建过程记录如下：
（1）pytorch安装
```shell
conda create --name openmmlab python=3.8 -y
conda activate openmmlab
conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.6 -c pytorch -c conda-forge
# 参照pytorch官网链接 https://pytorch.org/get-started/previous-versions/
```
（2）mmcv安装
```shell
pip install -U openmim
min install mmcv-full==1.6.0
# 参照 https://mmcv.readthedocs.io/en/latest/get_started/installation.html#
```
(3)mmdet安装
```shell
# 源码编译
git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
pip install -r requirements/build.txt
pip install -V -e . # or python setup.py develop

#pip安装
pip install mmdet -i https://pypi.tuna.tsinghua.edu.cn/simple
```
(4)apex安装--可选
```shell
# 源码编译
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir ./
```
(5)直接下载swin transformer官方文件夹比较方便得到configs文件
```shell
git clone https://github.com/SwinTransformer/Swin-Transformer-Object-Detection
```
(6)需要修改mmdet文件夹中的_init_.py文件 mmcv_maximum_version = 当前mmcv最高版本号
(7)新建checkpoints文件夹，下载对应config的pth文件

注意：**在demo文件夹中inference**会报错：
![](https://raw.githubusercontent.com/zjc0000/story_images/main/小书匠/1667301048750.png)
在主目录下则不会，原因未知

coco数据集标注详解：https://zhuanlan.zhihu.com/p/309549190
mmdetection框架详解：https://blog.csdn.net/qq_16137569/article/details/120929852



transformer:
self-attention:

multi-head self-attention:
多头注意力作用：
一个头就只有一个学习空间，多个头就有多个学习空间，可以学习到多个种类的相关性

positional encoding: 上述self-attention缺乏位置信息

cross-attention 与self-attention区别

embedding 向量化

224\*224 转变为 16\*16 为224/16=14倍下采样